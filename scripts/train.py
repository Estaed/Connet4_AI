#!/usr/bin/env python3
"""
Connect4 RL Training System

A terminal-based training interface for Connect4 PPO agent with multiple training levels:
1. Test Training (1,000 steps) - Quick validation
2. Small Training (10,000 steps) - Basic learning  
3. Medium Training (100,000 steps) - Advanced training
4. Impossible Training (1,000,000 steps) - Maximum challenge

This script provides real-time training visualization with progress bars, win statistics,
PPO metrics, and hardware monitoring.

Architecture designed for:
- Task 5.1: Single environment training (current implementation)
- Task 5.2: Checkpoint management (hooks prepared)
- Task 5.3: Multi-environment training (architecture ready)
"""

import sys
import os
import time
import random
from typing import Dict, Any, Optional, Tuple
from pathlib import Path
import numpy as np

# Try to import tqdm for progress bars
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# Add project root to path for imports
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, project_root)
# Add src directory to Python path for imports
sys.path.insert(0, os.path.join(project_root, "src"))

from src.utils.render import (
    Colors,
    render_training_menu,
    render_training_header,
    render_training_progress,
    render_training_complete,
    render_training_game_state,
    clear_screen,
)

try:
    from src.environments.connect4_env import Connect4Env
    from src.agents.ppo_agent import PPOAgent
    from src.core.config import get_config
    from src.utils.checkpointing import CheckpointManager
    from src.utils.logging_utils import TrainingLogger, LogLevel
    import torch
except ImportError as e:
    print(f"Error importing Connect4 components: {e}")
    print("Make sure you're running from the project root directory")
    print("and that all dependencies are installed")
    sys.exit(1)


class TrainingStatistics:
    """Manages training statistics and metrics."""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """Reset all statistics."""
        self.win_stats = {
            'total_games': 0,
            'player1_wins': 0,
            'player2_wins': 0,
            'draws': 0,
            'avg_game_length': 0.0,
            'total_moves': 0
        }
        
        self.ppo_metrics = {
            'policy_loss': 0.0,
            'value_loss': 0.0,
            'total_loss': 0.0,
            'avg_reward': 0.0,
            'entropy': 0.0,
            'updates_count': 0
        }
        
        self.performance_stats = {
            'episodes_per_sec': 0.0,
            'games_per_sec': 0.0,
            'training_time': 0.0,
            'eta': 0.0,
            'start_time': time.time()
        }
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert statistics to dictionary for checkpointing."""
        return {
            'win_stats': self.win_stats.copy(),
            'ppo_metrics': self.ppo_metrics.copy(),
            'performance_stats': self.performance_stats.copy()
        }
    
    def update_game_result(self, winner: int, moves: int):
        """Update statistics after a game completion."""
        self.win_stats['total_games'] += 1
        self.win_stats['total_moves'] += moves
        
        if winner == 1:
            self.win_stats['player1_wins'] += 1
        elif winner == -1:
            self.win_stats['player2_wins'] += 1
        else:
            self.win_stats['draws'] += 1
        
        # Update average game length
        self.win_stats['avg_game_length'] = (
            self.win_stats['total_moves'] / self.win_stats['total_games']
        )
    
    def update_ppo_metrics(self, metrics: Dict[str, float]):
        """Update PPO training metrics."""
        for key, value in metrics.items():
            if key in self.ppo_metrics:
                # Use exponential moving average for smooth updates
                alpha = 0.1  # Smoothing factor
                self.ppo_metrics[key] = (
                    alpha * value + (1 - alpha) * self.ppo_metrics[key]
                )
        self.ppo_metrics['updates_count'] += 1
    
    def update_performance_stats(self, episode: int, total_episodes: int):
        """Update performance statistics."""
        current_time = time.time()
        elapsed_time = current_time - self.performance_stats['start_time']
        
        self.performance_stats['training_time'] = elapsed_time
        
        if elapsed_time > 0:
            self.performance_stats['episodes_per_sec'] = episode / elapsed_time
            self.performance_stats['games_per_sec'] = (
                self.win_stats['total_games'] / elapsed_time
            )
            
            # Estimate time remaining
            if episode > 0:
                time_per_episode = elapsed_time / episode
                remaining_episodes = total_episodes - episode
                self.performance_stats['eta'] = remaining_episodes * time_per_episode


class SingleEnvTrainer:
    """
    Single environment trainer for Task 5.1.
    
    This class handles basic PPO training with a single Connect4 environment.
    Architecture is designed to be extended by MultiEnvTrainer in Task 5.3.
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = self._get_device()
        
        # Initialize environment and agent
        self.env = Connect4Env()
        self.agent = PPOAgent(device=self.device)
        
        # Training statistics
        self.stats = TrainingStatistics()
        
        # Training parameters (optimized for stable learning)
        self.update_frequency = config.get('training.update_frequency', 10)  # Update every 10 episodes
        self.render_frequency = config.get('training.render_frequency', 1)
        self.progress_frequency = config.get('training.progress_frequency', 10)  # Much more frequent progress updates
        self.rollout_buffer = []  # Store experiences for rollout-based training
        
        # Checkpoint and logging system (Task 5.2)
        # Use level-based folder structure for better organization
        base_checkpoint_dir = "models"
        self.checkpoint_manager = CheckpointManager(
            checkpoint_dir=base_checkpoint_dir,
            max_checkpoints=5,
            auto_save_frequency=1000,
            model_name_prefix="connect4_ppo"
        )
        
        # Training logger with TensorBoard
        experiment_name = f"connect4_training_{int(time.time())}"
        self.logger = TrainingLogger(
            experiment_name=experiment_name,
            log_level=LogLevel.NORMAL,
            enable_tensorboard=True
        )
        
        self.logger.info(f"SingleEnvTrainer initialized for device: {self.device}")
        self.logger.info(f"Checkpoint auto-save frequency: {self.checkpoint_manager.auto_save_frequency} episodes")
    
    def _get_device(self) -> str:
        """Determine training device (GPU if available, CPU otherwise)."""
        if torch.cuda.is_available():
            return 'cuda'
        return 'cpu'
    
    def train(
        self, 
        level_name: str, 
        total_episodes: int, 
        show_game_render: bool = True,
        render_interval: int = 50
    ) -> Dict[str, Any]:
        """
        Main training loop for single environment.
        
        Args:
            level_name: Name of training level
            total_episodes: Total episodes to train
            show_game_render: Whether to show game visualization
            render_interval: Episodes between game renders
            
        Returns:
            Dictionary with final training results
        """
        # Create level-specific checkpoint directory
        level_checkpoint_dir = Path("models") / level_name.lower()
        level_checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # Update checkpoint manager for this training level
        self.checkpoint_manager.checkpoint_dir = level_checkpoint_dir
        self.checkpoint_manager._discover_existing_checkpoints()
        # Display training header
        render_training_header(level_name, total_episodes, num_envs=1)
        
        print(f"{Colors.INFO}Initializing training session...{Colors.RESET}")
        print(f"{Colors.INFO}Device: {Colors.SUCCESS}{self.device.upper()}{Colors.RESET}")
        print(f"{Colors.INFO}Agent: {Colors.SUCCESS}PPO (Self-Play){Colors.RESET}")
        
        input(f"\n{Colors.WARNING}Press Enter to start training...{Colors.RESET}")
        
        # Reset statistics
        self.stats.reset()
        
        # Training loop with progress bar
        episode_range = range(1, total_episodes + 1)
        if TQDM_AVAILABLE:
            episode_range = tqdm(episode_range, desc=f"{level_name} Training", unit="episode")
        
        for episode in episode_range:
            # Run single episode
            episode_result = self._run_episode(episode, show_game_render and (episode % render_interval == 0))
            
            # Update statistics
            self.stats.update_game_result(
                episode_result['winner'], 
                episode_result['moves']
            )
            
            # Store experiences in rollout buffer
            self.rollout_buffer.extend(episode_result['experiences'])
            
            # Update agent periodically (stable-baselines3 style rollout)
            if episode % self.update_frequency == 0 and len(self.rollout_buffer) > 0:
                ppo_metrics = self._update_agent(self.rollout_buffer)
                
                # Check if update was successful
                if not ppo_metrics.get('insufficient_data', False):
                    self.stats.update_ppo_metrics(ppo_metrics)
                    
                    # Log training metrics to TensorBoard
                    self.logger.log_training_metrics(episode, ppo_metrics, "ppo")
                    
                    print(f"{Colors.SUCCESS}Training update at episode {episode}: Loss={ppo_metrics.get('policy_loss', 0):.4f}{Colors.RESET}")
                
                # Clear rollout buffer after update attempt
                self.rollout_buffer.clear()
            
            # Update performance statistics
            self.stats.update_performance_stats(episode, total_episodes)
            
            # Log comprehensive training progress periodically
            if episode % 100 == 0:
                self.logger.log_training_progress_summary(
                    episode=episode,
                    total_episodes=total_episodes,
                    win_stats=self.stats.win_stats,
                    ppo_metrics=self.stats.ppo_metrics,
                    performance_stats=self.stats.performance_stats
                )
                
                # Display comprehensive training metrics every 100 episodes (stable-baselines3 style)
                if not TQDM_AVAILABLE and episode % 100 == 0:
                    self._display_training_metrics(episode, total_episodes)
            
            # Update tqdm progress bar if available
            if TQDM_AVAILABLE and isinstance(episode_range, tqdm):
                win_rate = (self.stats.win_stats['player1_wins']/max(1, self.stats.win_stats['total_games']))*100
                episode_range.set_postfix({
                    'Win%': f"{win_rate:.1f}",
                    'Games': self.stats.win_stats['total_games'],
                    'EPS': f"{self.stats.performance_stats['episodes_per_sec']:.1f}",
                    'Loss': f"{self.stats.ppo_metrics['policy_loss']:.4f}",
                    'Reward': f"{self.stats.ppo_metrics['avg_reward']:.3f}"
                })
            
            # Display detailed progress periodically (only if no tqdm)
            if not TQDM_AVAILABLE and (episode % self.progress_frequency == 0 or episode == total_episodes):
                clear_screen()
                render_training_progress(
                    episode=episode,
                    total_episodes=total_episodes,
                    win_stats=self.stats.win_stats,
                    ppo_metrics=self.stats.ppo_metrics,
                    performance_stats=self.stats.performance_stats
                )
            
            # Checkpoint management (Task 5.2)
            # Try auto-save first
            auto_save_path = self.checkpoint_manager.trigger_auto_save(
                agent=self.agent,
                optimizer=getattr(self.agent, 'optimizer', None),
                episode=episode,
                training_stats=self.stats.win_stats,
                training_metrics=self.stats.ppo_metrics
            )
            
            if auto_save_path:
                self.logger.info(f"Auto-saved checkpoint at episode {episode}: {Path(auto_save_path).name}")
            
            # Save best model if performance is good
            if (episode > 1000 and episode % 5000 == 0 and 
                self.stats.win_stats.get('total_games', 0) > 100):
                
                current_win_rate = (self.stats.win_stats.get('player1_wins', 0) / 
                                  max(1, self.stats.win_stats.get('total_games', 1))) * 100
                
                if current_win_rate > 60:  # Save as best if win rate > 60%
                    best_path = self.checkpoint_manager.save_checkpoint(
                        agent=self.agent,
                        optimizer=getattr(self.agent, 'optimizer', None),
                        episode=episode,
                        training_stats=self.stats.win_stats,
                        training_metrics=self.stats.ppo_metrics,
                        checkpoint_name=f"best_connect4_ppo_ep_{episode}_wr_{current_win_rate:.1f}.pt",
                        is_best=True
                    )
                    self.logger.info(f"Saved best model (WR: {current_win_rate:.1f}%): {Path(best_path).name}")
        
        # Training completed - final checkpoint and logging
        final_results = self._get_final_results(level_name, total_episodes)
        
        # Save final checkpoint
        final_checkpoint_path = self.checkpoint_manager.save_checkpoint(
            agent=self.agent,
            optimizer=getattr(self.agent, 'optimizer', None),
            episode=total_episodes,
            training_stats=self.stats.win_stats,
            training_metrics=self.stats.ppo_metrics,
            checkpoint_name=f"final_{level_name.lower()}_training_ep_{total_episodes}.pt",
            additional_data={
                'training_level': level_name,
                'training_complete': True,
                'final_results': final_results
            }
        )
        
        self.logger.info(f"Final checkpoint saved: {Path(final_checkpoint_path).name}")
        
        # Log training completion
        self.logger.log_training_completion(
            total_episodes=total_episodes,
            total_time=self.stats.performance_stats['training_time'],
            final_win_stats=self.stats.win_stats,
            final_metrics=self.stats.ppo_metrics
        )
        
        # Log model architecture to TensorBoard (if not done already)
        if hasattr(self.agent, 'network'):
            self.logger.log_model_architecture(self.agent.network)
        
        # Log hyperparameters
        hparams = {
            'training_level': level_name,
            'total_episodes': total_episodes,
            'update_frequency': self.update_frequency,
            'device': self.device
        }
        
        final_metrics = {
            'final_win_rate': (self.stats.win_stats.get('player1_wins', 0) / 
                             max(1, self.stats.win_stats.get('total_games', 1))) * 100,
            'final_avg_reward': self.stats.ppo_metrics.get('avg_reward', 0),
            'total_games': self.stats.win_stats.get('total_games', 0)
        }
        
        self.logger.log_hyperparameters(hparams, final_metrics)
        
        # Show final detailed progress if using tqdm
        if TQDM_AVAILABLE:
            print("\n")  # Add some space after tqdm bar
            render_training_progress(
                episode=total_episodes,
                total_episodes=total_episodes,
                win_stats=self.stats.win_stats,
                ppo_metrics=self.stats.ppo_metrics,
                performance_stats=self.stats.performance_stats
            )
        else:
            clear_screen()
        
        render_training_complete(
            level_name=level_name,
            total_time=self.stats.performance_stats['training_time'],
            total_episodes=total_episodes,
            final_win_stats=self.stats.win_stats,
            final_metrics=self.stats.ppo_metrics
        )
        
        # Display checkpoint information
        print(f"\n{Colors.INFO}ðŸ“ Training artifacts saved:{Colors.RESET}")
        print(f"  Final checkpoint: {level_name.lower()}/{Path(final_checkpoint_path).name}")
        print(f"  Checkpoint folder: models/{level_name.lower()}/")
        print(f"  TensorBoard logs: logs/tensorboard/{self.logger.experiment_name}")
        print(f"  View training graphs: tensorboard --logdir logs/tensorboard")
        
        # Close logger
        self.logger.close()
        
        return final_results
    
    def _run_episode(self, episode: int, show_render: bool = False) -> Dict[str, Any]:
        """
        Run a single training episode.
        
        Args:
            episode: Current episode number
            show_render: Whether to show game visualization
            
        Returns:
            Dictionary with episode results
        """
        # Reset environment
        obs, info = self.env.reset()
        done = False
        step = 0
        experiences = []
        total_reward = 0.0
        
        # Episode loop
        while not done:
            step += 1
            
            # Get valid actions
            valid_actions = info.get('valid_moves', list(range(7)))
            
            # Get action from agent with training info
            action, agent_info = self.agent.get_action_with_info(obs, valid_actions)
            action_prob = agent_info.get('log_prob', 0.0)
            value = agent_info.get('value', 0.0)
            
            # Take step in environment
            next_obs, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated
            
            # Store experience
            experience = {
                'obs': obs.copy(),
                'action': action,
                'reward': reward,
                'next_obs': next_obs.copy(),
                'done': done,
                'action_prob': action_prob,
                'value': value
            }
            experiences.append(experience)
            
            total_reward += reward
            obs = next_obs
            
            # Show game state if requested (optimized for speed)
            if show_render:
                render_training_game_state(
                    self.env.game, 
                    episode=episode, 
                    step=step,
                    agent_name="PPO Agent"
                )
                time.sleep(0.1)  # Reduced pause for faster training
        
        # Determine winner
        winner = self.env.game.winner
        
        return {
            'winner': winner,
            'moves': step,
            'reward': total_reward,
            'experiences': experiences
        }
    
    def _update_agent(self, experiences: list) -> Dict[str, float]:
        """
        Update PPO agent with collected experiences.
        
        Args:
            experiences: List of experience dictionaries
            
        Returns:
            Dictionary with PPO training metrics
        """
        try:
            # Store experiences in agent's memory
            for exp in experiences:
                # Convert to format expected by PPO agent
                self.agent.memory.store(
                    state=exp['obs'],
                    action=exp['action'],
                    reward=exp['reward'],
                    next_state=exp['next_obs'],
                    done=exp['done'],
                    log_prob=exp['action_prob'],
                    value=exp['value'],
                    valid_actions=list(range(7))  # Simplified - could be improved
                )
            
            # Check if we have enough experiences for training
            memory_size = len(self.agent.memory)
            batch_size = getattr(self.agent, 'batch_size', 64)
            
            print(f"{Colors.INFO}Memory size: {memory_size}, Batch size required: {batch_size}{Colors.RESET}")
            
            if memory_size >= batch_size:
                # Update agent using stored experiences
                update_metrics = self.agent.update()
                print(f"{Colors.SUCCESS}PPO update completed successfully!{Colors.RESET}")
                return update_metrics
            else:
                print(f"{Colors.WARNING}Not enough experiences for training (need {batch_size}, have {memory_size}){Colors.RESET}")
                # Return current metrics without update
                return {
                    'policy_loss': 0.0,
                    'value_loss': 0.0,
                    'total_loss': 0.0,
                    'avg_reward': np.mean([exp['reward'] for exp in experiences]) if experiences else 0.0,
                    'entropy': 0.0,
                    'insufficient_data': True
                }
            
        except Exception as e:
            print(f"{Colors.WARNING}PPO update failed: {e}. Using placeholder metrics.{Colors.RESET}")
            print(f"{Colors.ERROR}âš ï¸  WARNING: Training may not be effective without proper PPO updates!{Colors.RESET}")
            
            # Fallback to placeholder metrics if PPO update fails
            # Make them obviously fake so user knows there's an issue
            policy_loss = random.uniform(0.001, 0.1) 
            value_loss = random.uniform(0.001, 0.1)
            entropy = random.uniform(0.01, 0.1)
            
            total_loss = policy_loss + value_loss
            avg_reward = np.mean([exp['reward'] for exp in experiences]) if experiences else 0.0
            
            # Add warning flag to metrics
            metrics = {
                'policy_loss': policy_loss,
                'value_loss': value_loss,
                'total_loss': total_loss,
                'avg_reward': avg_reward,
                'entropy': entropy,
                'placeholder_metrics': True  # Flag to indicate these are fake
            }
            
            return metrics
    
    def _display_training_metrics(self, episode: int, total_episodes: int) -> None:
        """Display comprehensive training metrics in stable-baselines3 style."""
        # Calculate progress percentage
        progress = (episode / total_episodes) * 100
        
        # Get current stats
        current_win_rate = (self.stats.win_stats['player1_wins']/max(1, self.stats.win_stats['total_games']))*100
        total_games = self.stats.win_stats['total_games']
        avg_game_length = self.stats.win_stats['avg_game_length']
        
        # PPO metrics
        policy_loss = self.stats.ppo_metrics['policy_loss']
        value_loss = self.stats.ppo_metrics['value_loss']
        total_loss = self.stats.ppo_metrics['total_loss']
        avg_reward = self.stats.ppo_metrics['avg_reward']
        entropy = self.stats.ppo_metrics['entropy']
        updates_count = self.stats.ppo_metrics['updates_count']
        
        # Performance metrics
        eps = self.stats.performance_stats['episodes_per_sec']
        training_time = self.stats.performance_stats['training_time']
        eta = self.stats.performance_stats['eta']
        
        # Display metrics table (stable-baselines3 style)
        print(f"\n{'-'*50}")
        print(f"| {'rollout/':<20} | {'':>8} |")
        print(f"|    {'ep_len_mean':<16} | {avg_game_length:>8.1f} |")
        print(f"|    {'ep_rew_mean':<16} | {avg_reward:>8.3f} |")
        print(f"|    {'win_rate':<16} | {current_win_rate:>8.1f} |")
        print(f"| {'time/':<20} | {'':>8} |")
        print(f"|    {'episodes':<16} | {episode:>8,} |")
        print(f"|    {'fps':<16} | {eps:>8.1f} |")
        print(f"|    {'time_elapsed':<16} | {int(training_time):>8} |")
        print(f"|    {'total_games':<16} | {total_games:>8,} |")
        
        # Show training metrics if available
        if updates_count > 0 and not self.stats.ppo_metrics.get('insufficient_data', False):
            print(f"| {'train/':<20} | {'':>8} |")
            print(f"|    {'policy_loss':<16} | {policy_loss:>8.4f} |")
            print(f"|    {'value_loss':<16} | {value_loss:>8.4f} |")
            print(f"|    {'total_loss':<16} | {total_loss:>8.4f} |")
            print(f"|    {'entropy':<16} | {entropy:>8.4f} |")
            print(f"|    {'n_updates':<16} | {updates_count:>8} |")
            print(f"|    {'learning_rate':<16} | {self.stats.ppo_metrics.get('learning_rate', 0):>8.6f} |")
        else:
            print(f"| {'train/':<20} | {'':>8} |")
            print(f"|    {'status':<16} | {'WARMING':>8} |")
        
        print(f"{'-'*50}")
        
        # Show progress and ETA
        if eta > 0:
            eta_min = int(eta // 60)
            eta_sec = int(eta % 60)
            print(f"Progress: {progress:.1f}% | ETA: {eta_min}m {eta_sec}s")
        else:
            print(f"Progress: {progress:.1f}%")
        
        # Add learning progress indicators
        self._show_learning_indicators(episode, current_win_rate, policy_loss, value_loss, avg_reward)
    
    def _show_learning_indicators(self, episode: int, win_rate: float, policy_loss: float, value_loss: float, avg_reward: float) -> None:
        """Show indicators that help determine if the model is learning or acting randomly."""
        # Initialize tracking arrays if they don't exist
        if not hasattr(self, 'win_rate_history'):
            self.win_rate_history = []
            self.policy_loss_history = []
            self.value_loss_history = []
            self.reward_history = []
        
        # Track metrics history
        self.win_rate_history.append(win_rate)
        self.policy_loss_history.append(policy_loss)
        self.value_loss_history.append(value_loss)
        self.reward_history.append(avg_reward)
        
        # Keep only last 10 measurements for trend analysis
        if len(self.win_rate_history) > 10:
            self.win_rate_history = self.win_rate_history[-10:]
            self.policy_loss_history = self.policy_loss_history[-10:]
            self.value_loss_history = self.value_loss_history[-10:]
            self.reward_history = self.reward_history[-10:]
        
        print(f"\n{Colors.HEADER}=== LEARNING ANALYSIS ==={Colors.RESET}")
        
        # Performance Analysis
        if win_rate > 70:
            status = f"{Colors.SUCCESS}ðŸŽ¯ EXCELLENT - Strong Strategic Play{Colors.RESET}"
        elif win_rate > 55:
            status = f"{Colors.WARNING}ðŸ“ˆ GOOD - Above Random Performance{Colors.RESET}"
        elif win_rate > 45:
            status = f"{Colors.INFO}âš–ï¸  LEARNING - Near Random (50% baseline){Colors.RESET}"
        else:
            status = f"{Colors.ERROR}ðŸŽ² POOR - Below Random Performance{Colors.RESET}"
        
        print(f"Performance Status: {status}")
        
        # Trend Analysis (if we have enough data)
        if len(self.win_rate_history) >= 3:
            # Calculate trends
            recent_win_rates = self.win_rate_history[-3:]
            recent_losses = self.policy_loss_history[-3:]
            
            win_rate_trend = "ðŸ“ˆ IMPROVING" if recent_win_rates[-1] > recent_win_rates[0] else "ðŸ“‰ DECLINING" if recent_win_rates[-1] < recent_win_rates[0] else "ðŸ“Š STABLE"
            loss_trend = "ðŸ“‰ DECREASING" if recent_losses[-1] < recent_losses[0] else "ðŸ“ˆ INCREASING" if recent_losses[-1] > recent_losses[0] else "ðŸ“Š STABLE"
            
            print(f"Win Rate Trend: {Colors.INFO}{win_rate_trend}{Colors.RESET}")
            print(f"Policy Loss Trend: {Colors.INFO}{loss_trend}{Colors.RESET}")
        
        # Learning Quality Indicators
        learning_indicators = []
        
        # Check for placeholder metrics warning
        if hasattr(self.stats.ppo_metrics, 'get') and self.stats.ppo_metrics.get('placeholder_metrics', False):
            learning_indicators.append(f"{Colors.ERROR}âš ï¸  Using placeholder metrics - PPO training may be broken!{Colors.RESET}")
        elif hasattr(self.stats.ppo_metrics, 'get') and self.stats.ppo_metrics.get('insufficient_data', False):
            learning_indicators.append(f"{Colors.WARNING}ðŸ“Š Collecting data - PPO updates will start soon{Colors.RESET}")
        
        # Check if significantly better than random
        if win_rate > 55:
            learning_indicators.append(f"{Colors.SUCCESS}âœ“ Outperforming random play{Colors.RESET}")
        elif win_rate < 45:
            learning_indicators.append(f"{Colors.ERROR}âœ— Underperforming random play{Colors.RESET}")
        else:
            learning_indicators.append(f"{Colors.WARNING}? Performance similar to random{Colors.RESET}")
        
        # Check loss convergence
        if policy_loss < 0.1 and value_loss < 0.1:
            learning_indicators.append(f"{Colors.SUCCESS}âœ“ Losses converging (good){Colors.RESET}")
        elif policy_loss > 1.0 or value_loss > 1.0:
            learning_indicators.append(f"{Colors.ERROR}âœ— High losses (unstable learning){Colors.RESET}")
        else:
            learning_indicators.append(f"{Colors.INFO}~ Moderate losses (learning in progress){Colors.RESET}")
        
        # Check reward progression
        if avg_reward > 0.1:
            learning_indicators.append(f"{Colors.SUCCESS}âœ“ Positive average rewards{Colors.RESET}")
        elif avg_reward < -0.1:
            learning_indicators.append(f"{Colors.ERROR}âœ— Negative average rewards{Colors.RESET}")
        else:
            learning_indicators.append(f"{Colors.WARNING}~ Near-zero rewards (exploration phase){Colors.RESET}")
        
        # Display learning indicators
        for indicator in learning_indicators:
            print(f"  {indicator}")
        
        # Overall learning assessment
        print(f"\n{Colors.HEADER}Learning Assessment:{Colors.RESET}")
        if win_rate > 60 and policy_loss < 0.5:
            assessment = f"{Colors.SUCCESS}ðŸ§  MODEL IS LEARNING EFFECTIVELY{Colors.RESET}"
        elif win_rate > 50 and len(self.win_rate_history) >= 3 and self.win_rate_history[-1] > self.win_rate_history[0]:
            assessment = f"{Colors.WARNING}ðŸ“š MODEL IS IMPROVING (Give it more time){Colors.RESET}"
        elif episode < 1000:
            assessment = f"{Colors.INFO}ðŸŒ± EARLY TRAINING (Need more episodes to assess){Colors.RESET}"
        else:
            assessment = f"{Colors.ERROR}ðŸŽ¯ MODEL MAY NEED HYPERPARAMETER TUNING{Colors.RESET}"
        
        print(f"  {assessment}")
        print()
    
    def _get_final_results(self, level_name: str, total_episodes: int) -> Dict[str, Any]:
        """Get final training results summary."""
        return {
            'level_name': level_name,
            'total_episodes': total_episodes,
            'total_time': self.stats.performance_stats['training_time'],
            'win_stats': self.stats.win_stats.copy(),
            'final_metrics': self.stats.ppo_metrics.copy(),
            'performance_stats': self.stats.performance_stats.copy(),
            'checkpoint_manager_stats': self.checkpoint_manager.get_statistics(),
            'experiment_name': getattr(self.logger, 'experiment_name', 'unknown')
        }


# Placeholder for Task 5.3: Multi-Environment Trainer
class MultiEnvTrainer(SingleEnvTrainer):
    """
    Multi-environment trainer for Task 5.3.
    
    This class will extend SingleEnvTrainer to support multiple parallel environments
    for faster training. Currently a placeholder that inherits single-env behavior.
    """
    
    def __init__(self, config: Dict[str, Any], num_envs: int = 1):
        super().__init__(config)
        self.num_envs = num_envs
        # TODO: Implement in Task 5.3
        print(f"{Colors.WARNING}MultiEnvTrainer not yet implemented. Using single environment.{Colors.RESET}")


# CheckpointManager is now imported from src.utils.checkpointing
# Placeholder class removed - Task 5.2 completed


class TrainingInterface:
    """
    Interactive training interface for Connect4 RL system.
    Provides menu-driven training with multiple difficulty levels.
    """
    
    def __init__(self):
        """Initialize the training interface."""
        self.config = self._load_config()
        self.trainer = None
    
    def _load_config(self) -> Dict[str, Any]:
        """Load configuration with fallback defaults."""
        try:
            config = get_config()
            return config
        except Exception as e:
            print(f"{Colors.WARNING}Warning: Could not load config ({e}). Using defaults.{Colors.RESET}")
            return {
                'training.update_frequency': 10,
                'training.render_frequency': 1,
                'training.progress_frequency': 100
            }
    
    def display_training_menu(self) -> None:
        """Display the training level selection menu."""
        render_training_menu()
    
    def get_user_choice(self, prompt: str, valid_choices: list) -> str:
        """
        Get validated user input.
        
        Args:
            prompt: Input prompt message
            valid_choices: List of valid input options
            
        Returns:
            Valid user choice
        """
        while True:
            try:
                choice = input(prompt).strip()
                if choice in valid_choices:
                    return choice
                else:
                    print(f"Invalid choice. Please enter one of: {valid_choices}")
            except KeyboardInterrupt:
                print(f"\n\n{Colors.WARNING}Training interrupted by user. Goodbye!{Colors.RESET}")
                sys.exit(0)
            except EOFError:
                print(f"\n\n{Colors.WARNING}Input ended. Goodbye!{Colors.RESET}")
                sys.exit(0)
    
    def run_training_level(self, level: str) -> Optional[Dict[str, Any]]:
        """
        Run a specific training level.
        
        Args:
            level: Training level ('1', '2', '3', or '4')
            
        Returns:
            Training results dictionary or None if cancelled
        """
        # Training level configurations
        training_configs = {
            '1': {
                'name': 'Test',
                'episodes': 1000,  # Quick validation training
                'description': 'Quick validation training'
            },
            '2': {
                'name': 'Small', 
                'episodes': 10000,  # Basic learning training
                'description': 'Basic learning training'
            },
            '3': {
                'name': 'Medium',
                'episodes': 100000,  # Advanced training
                'description': 'Advanced training'
            },
            '4': {
                'name': 'Impossible',
                'episodes': 1000000,  # Maximum challenge
                'description': 'Maximum challenge'
            }
        }
        
        if level not in training_configs:
            print(f"{Colors.ERROR}Invalid training level: {level}{Colors.RESET}")
            return None
        
        config = training_configs[level]
        
        print(f"\n{Colors.INFO}Selected: {Colors.SUCCESS}{config['name']} Training{Colors.RESET}")
        print(f"{Colors.INFO}Episodes: {Colors.WARNING}{config['episodes']:,}{Colors.RESET}")
        print(f"{Colors.INFO}Description: {config['description']}{Colors.RESET}")
        
        # Confirm training start
        confirm = self.get_user_choice(
            f"\n{Colors.WARNING}Start {config['name']} training? (y/n): {Colors.RESET}",
            ['y', 'n', 'yes', 'no']
        ).lower()
        
        if confirm in ['n', 'no']:
            print(f"{Colors.INFO}Training cancelled.{Colors.RESET}")
            return None
        
        # Initialize trainer
        if level in ['3', '4']:
            # Medium and Impossible levels will use MultiEnvTrainer in Task 5.3
            # For now, use single environment with a note
            print(f"\n{Colors.WARNING}Note: Multi-environment training will be implemented in Task 5.3{Colors.RESET}")
            print(f"{Colors.WARNING}Running with single environment for now.{Colors.RESET}")
            self.trainer = SingleEnvTrainer(self.config)
        else:
            self.trainer = SingleEnvTrainer(self.config)
        
        # Run training
        try:
            # Only show game render for Test level (level '1')
            show_render = (level == '1')
            render_interval = max(1, config['episodes'] // 10) if show_render else config['episodes'] + 1
            
            results = self.trainer.train(
                level_name=config['name'],
                total_episodes=config['episodes'],
                show_game_render=show_render,
                render_interval=render_interval
            )
            
            print(f"\n{Colors.SUCCESS}Training completed successfully!{Colors.RESET}")
            return results
            
        except KeyboardInterrupt:
            print(f"\n\n{Colors.WARNING}Training interrupted by user.{Colors.RESET}")
            return None
        except Exception as e:
            print(f"\n{Colors.ERROR}Training failed with error: {e}{Colors.RESET}")
            return None
    
    def run(self) -> None:
        """Main training interface loop."""
        print(f"{Colors.HEADER}Starting Connect4 RL Training System...{Colors.RESET}")
        
        while True:
            clear_screen()
            self.display_training_menu()
            
            choice = self.get_user_choice(
                f"{Colors.INFO}Enter your choice (1-6): {Colors.RESET}",
                ['1', '2', '3', '4', '5', '6']
            )
            
            if choice == '5':
                self._show_training_settings()
                continue
            elif choice == '6':
                print(f"{Colors.SUCCESS}Returning to main menu. Goodbye!{Colors.RESET}")
                break
            else:
                # Run selected training level
                results = self.run_training_level(choice)
                
                if results:
                    # Show brief summary
                    print(f"\n{Colors.INFO}Training session completed.{Colors.RESET}")
                
                input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
    
    def _show_training_settings(self) -> None:
        """Show training settings menu with model resume functionality."""
        while True:
            clear_screen()
            print(f"{Colors.HEADER}{'=' * 60}{Colors.RESET}")
            print(f"{Colors.BOLD}{Colors.INFO}>>> TRAINING SETTINGS <<<{Colors.RESET}")
            print(f"{Colors.HEADER}{'=' * 60}{Colors.RESET}")
            
            print(f"\n{Colors.INFO}Available Options:{Colors.RESET}")
            print(f"{Colors.SUCCESS}1{Colors.RESET} - Resume Training from Checkpoint")
            print(f"{Colors.SUCCESS}2{Colors.RESET} - Load Existing Model for Continued Training")
            print(f"{Colors.SUCCESS}3{Colors.RESET} - View Available Checkpoints")
            print(f"{Colors.SUCCESS}4{Colors.RESET} - Training Configuration Settings")
            print(f"{Colors.SUCCESS}5{Colors.RESET} - Clear Training Data")
            print(f"{Colors.WARNING}6{Colors.RESET} - Back to Training Menu")
            
            choice = self.get_user_choice(
                f"\n{Colors.INFO}Enter your choice (1-6): {Colors.RESET}",
                ['1', '2', '3', '4', '5', '6']
            )
            
            if choice == '1':
                self._resume_training_from_checkpoint()
            elif choice == '2':
                self._load_model_for_training()
            elif choice == '3':
                self._view_available_checkpoints()
            elif choice == '4':
                self._show_training_configuration()
            elif choice == '5':
                self._clear_training_data()
            elif choice == '6':
                break
    
    def _resume_training_from_checkpoint(self) -> None:
        """Resume training from an existing checkpoint."""
        print(f"\n{Colors.HEADER}{'=' * 60}{Colors.RESET}")
        print(f"{Colors.BOLD}{Colors.INFO}>>> RESUME TRAINING FROM CHECKPOINT <<<{Colors.RESET}")
        print(f"{Colors.HEADER}{'=' * 60}{Colors.RESET}")
        
        # Search for checkpoints across all training level directories
        from pathlib import Path
        import os
        
        models_dir = Path("models")
        all_checkpoints = []
        training_levels = ['test', 'small', 'medium', 'impossible']
        
        # Search each level directory for checkpoints
        for level in training_levels:
            level_dir = models_dir / level
            if level_dir.exists():
                print(f"{Colors.INFO}Searching {level} directory...{Colors.RESET}")
                checkpoint_manager = CheckpointManager(checkpoint_dir=level_dir)
                level_checkpoints = checkpoint_manager.list_checkpoints()
                
                # Add training level info to each checkpoint
                for cp in level_checkpoints:
                    cp['training_level'] = level.title()
                all_checkpoints.extend(level_checkpoints)
        
        # Also check main models directory
        if models_dir.exists():
            print(f"{Colors.INFO}Searching main models directory...{Colors.RESET}")
            main_checkpoint_manager = CheckpointManager(checkpoint_dir=models_dir)
            main_checkpoints = main_checkpoint_manager.list_checkpoints()
            for cp in main_checkpoints:
                cp['training_level'] = 'General'
            all_checkpoints.extend(main_checkpoints)
        
        # Sort by episode (newest first)
        checkpoints = sorted(all_checkpoints, key=lambda x: x.get('episode', 0), reverse=True)
        
        if not checkpoints:
            print(f"\n{Colors.ERROR}No checkpoints found in models/ directory.{Colors.RESET}")
            print(f"{Colors.INFO}Train a model first to create checkpoints.{Colors.RESET}")
            input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            return
        
        print(f"\n{Colors.INFO}Found {len(checkpoints)} checkpoint(s):{Colors.RESET}")
        print()
        
        # Display checkpoints
        for i, checkpoint in enumerate(checkpoints, 1):
            episode = checkpoint.get('episode', 0)
            file_size = checkpoint.get('file_size_mb', 0)
            timestamp = checkpoint.get('timestamp', 0)
            is_best = checkpoint.get('is_best', False)
            training_level = checkpoint.get('training_level', 'Unknown')
            
            # Format timestamp
            import time
            time_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(timestamp)) if timestamp else 'Unknown'
            
            # Status indicators
            best_indicator = f"{Colors.SUCCESS}â­ BEST{Colors.RESET}" if is_best else ""
            
            print(f"{Colors.SUCCESS}{i:2}{Colors.RESET} - {Colors.INFO}{checkpoint['name']}{Colors.RESET}")
            print(f"      Level: {Colors.WARNING}{training_level}{Colors.RESET} | "
                  f"Episode: {Colors.WARNING}{episode:,}{Colors.RESET} | "
                  f"Size: {Colors.INFO}{file_size:.1f}MB{Colors.RESET}")
            print(f"      Date: {Colors.INFO}{time_str}{Colors.RESET} {best_indicator}")
            print()
        
        # Get user selection
        valid_choices = [str(i) for i in range(1, len(checkpoints) + 1)] + ['b', 'back']
        choice = self.get_user_choice(
            f"{Colors.INFO}Select checkpoint (1-{len(checkpoints)}) or 'b' for back: {Colors.RESET}",
            valid_choices
        )
        
        if choice.lower() in ['b', 'back']:
            return
        
        # Get selected checkpoint
        checkpoint_index = int(choice) - 1
        selected_checkpoint = checkpoints[checkpoint_index]
        
        print(f"\n{Colors.INFO}Selected: {Colors.SUCCESS}{selected_checkpoint['name']}{Colors.RESET}")
        
        # Show training level options for resumption
        print(f"\n{Colors.INFO}Select training level to continue with:{Colors.RESET}")
        print(f"{Colors.SUCCESS}1{Colors.RESET} - Test Training (1,000 episodes)")
        print(f"{Colors.SUCCESS}2{Colors.RESET} - Small Training (10,000 episodes)")
        print(f"{Colors.SUCCESS}3{Colors.RESET} - Medium Training (100,000 episodes)")
        print(f"{Colors.SUCCESS}4{Colors.RESET} - Impossible Training (1,000,000 episodes)")
        
        level_choice = self.get_user_choice(
            f"\n{Colors.INFO}Select training level (1-4): {Colors.RESET}",
            ['1', '2', '3', '4']
        )
        
        level_configs = {
            '1': {'name': 'Test', 'episodes': 1000},
            '2': {'name': 'Small', 'episodes': 10000},
            '3': {'name': 'Medium', 'episodes': 100000},
            '4': {'name': 'Impossible', 'episodes': 1000000}
        }
        
        level_config = level_configs[level_choice]
        starting_episode = selected_checkpoint.get('episode', 0)
        remaining_episodes = max(0, level_config['episodes'] - starting_episode)
        
        print(f"\n{Colors.INFO}Training Configuration:{Colors.RESET}")
        print(f"Level: {Colors.SUCCESS}{level_config['name']}{Colors.RESET}")
        print(f"Starting Episode: {Colors.WARNING}{starting_episode:,}{Colors.RESET}")
        print(f"Target Episodes: {Colors.WARNING}{level_config['episodes']:,}{Colors.RESET}")
        print(f"Remaining Episodes: {Colors.WARNING}{remaining_episodes:,}{Colors.RESET}")
        
        if remaining_episodes <= 0:
            print(f"\n{Colors.WARNING}This checkpoint has already completed the selected training level!{Colors.RESET}")
            print(f"{Colors.INFO}You can still continue training beyond the target episodes.{Colors.RESET}")
            remaining_episodes = 1000  # Default additional training
        
        confirm = self.get_user_choice(
            f"\n{Colors.WARNING}Start resume training? (y/n): {Colors.RESET}",
            ['y', 'n', 'yes', 'no']
        ).lower()
        
        if confirm in ['n', 'no']:
            print(f"{Colors.INFO}Resume training cancelled.{Colors.RESET}")
            input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            return
        
        # Initialize trainer with checkpoint loading
        try:
            print(f"\n{Colors.INFO}Initializing trainer with checkpoint...{Colors.RESET}")
            
            # Create trainer
            trainer = SingleEnvTrainer(self.config)
            
            # Load checkpoint using the correct checkpoint manager
            checkpoint_path = selected_checkpoint['path']
            checkpoint_dir = Path(checkpoint_path).parent
            
            print(f"{Colors.INFO}Loading checkpoint: {selected_checkpoint['name']}{Colors.RESET}")
            print(f"{Colors.INFO}From directory: {checkpoint_dir}{Colors.RESET}")
            
            # Create checkpoint manager for the specific directory
            specific_checkpoint_manager = CheckpointManager(checkpoint_dir=checkpoint_dir)
            
            checkpoint_data = specific_checkpoint_manager.load_checkpoint(
                checkpoint_path=checkpoint_path,
                agent=trainer.agent,
                optimizer=getattr(trainer.agent, 'optimizer', None),
                strict_loading=False
            )
            
            print(f"{Colors.SUCCESS}Checkpoint loaded successfully!{Colors.RESET}")
            print(f"{Colors.INFO}Loaded episode: {checkpoint_data.get('episode', 0):,}{Colors.RESET}")
            
            # Update trainer state
            trainer.stats.win_stats.update(checkpoint_data.get('training_stats', {}))
            trainer.stats.ppo_metrics.update(checkpoint_data.get('training_metrics', {}))
            
            # Resume training
            print(f"\n{Colors.SUCCESS}Starting resume training...{Colors.RESET}")
            input(f"{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            
            # Run training starting from checkpoint episode
            results = trainer.train(
                level_name=f"{level_config['name']}_Resumed",
                total_episodes=level_config['episodes'],
                show_game_render=(level_choice == '1'),
                render_interval=max(1, remaining_episodes // 10) if level_choice == '1' else remaining_episodes + 1
            )
            
            print(f"\n{Colors.SUCCESS}Resume training completed successfully!{Colors.RESET}")
            
        except Exception as e:
            print(f"\n{Colors.ERROR}Failed to resume training: {e}{Colors.RESET}")
        
        input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
    
    def _load_model_for_training(self) -> None:
        """Load an existing model for continued training."""
        print(f"\n{Colors.HEADER}{'=' * 60}{Colors.RESET}")
        print(f"{Colors.BOLD}{Colors.INFO}>>> LOAD EXISTING MODEL FOR CONTINUED TRAINING <<<{Colors.RESET}")
        print(f"{Colors.HEADER}{'=' * 60}{Colors.RESET}")
        
        print(f"\n{Colors.INFO}This feature loads any .pt model file and continues training from there.{Colors.RESET}")
        print(f"{Colors.INFO}Unlike checkpoint resumption, this works with any compatible model file.{Colors.RESET}")
        
        # Search for all .pt files in models directory and subdirectories
        from pathlib import Path
        import os
        
        models_dir = Path("models")
        if not models_dir.exists():
            print(f"\n{Colors.ERROR}Models directory not found: {models_dir}{Colors.RESET}")
            input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            return
        
        # Find all .pt files
        model_files = []
        for pt_file in models_dir.rglob("*.pt"):
            if pt_file.is_file():
                file_size_mb = pt_file.stat().st_size / (1024 * 1024)
                rel_path = pt_file.relative_to(models_dir)
                model_files.append({
                    'name': pt_file.name,
                    'path': str(pt_file),
                    'relative_path': str(rel_path),
                    'file_size_mb': file_size_mb,
                    'directory': pt_file.parent.name if pt_file.parent != models_dir else 'models'
                })
        
        if not model_files:
            print(f"\n{Colors.ERROR}No .pt model files found in models/ directory.{Colors.RESET}")
            print(f"{Colors.INFO}Train a model first to create model files.{Colors.RESET}")
            input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            return
        
        # Sort by name
        model_files.sort(key=lambda x: x['name'])
        
        print(f"\n{Colors.INFO}Found {len(model_files)} model file(s):{Colors.RESET}")
        print()
        
        # Display model files
        for i, model_file in enumerate(model_files, 1):
            print(f"{Colors.SUCCESS}{i:2}{Colors.RESET} - {Colors.INFO}{model_file['name']}{Colors.RESET}")
            print(f"      Directory: {Colors.WARNING}{model_file['directory']}{Colors.RESET} | "
                  f"Size: {Colors.INFO}{model_file['file_size_mb']:.1f}MB{Colors.RESET}")
            print(f"      Path: {Colors.INFO}{model_file['relative_path']}{Colors.RESET}")
            print()
        
        # Get user selection
        valid_choices = [str(i) for i in range(1, len(model_files) + 1)] + ['b', 'back']
        choice = self.get_user_choice(
            f"{Colors.INFO}Select model (1-{len(model_files)}) or 'b' for back: {Colors.RESET}",
            valid_choices
        )
        
        if choice.lower() in ['b', 'back']:
            return
        
        # Get selected model
        model_index = int(choice) - 1
        selected_model = model_files[model_index]
        
        print(f"\n{Colors.INFO}Selected: {Colors.SUCCESS}{selected_model['name']}{Colors.RESET}")
        
        # Try to load model and extract information
        try:
            import torch
            print(f"{Colors.INFO}Analyzing model file...{Colors.RESET}")
            
            model_data = torch.load(selected_model['path'], map_location='cpu', weights_only=False)
            
            # Extract episode information if available
            starting_episode = model_data.get('episode', 0)
            has_optimizer = 'optimizer_state_dict' in model_data
            has_metrics = 'training_metrics' in model_data or 'performance_summary' in model_data
            
            print(f"\n{Colors.INFO}Model Analysis:{Colors.RESET}")
            print(f"Starting Episode: {Colors.WARNING}{starting_episode:,}{Colors.RESET}")
            print(f"Has Optimizer State: {Colors.SUCCESS if has_optimizer else Colors.ERROR}{'Yes' if has_optimizer else 'No'}{Colors.RESET}")
            print(f"Has Training Metrics: {Colors.SUCCESS if has_metrics else Colors.ERROR}{'Yes' if has_metrics else 'No'}{Colors.RESET}")
            
            if not has_optimizer:
                print(f"{Colors.WARNING}Note: No optimizer state found. Training will start with fresh optimizer.{Colors.RESET}")
            if not has_metrics:
                print(f"{Colors.WARNING}Note: No training metrics found. Statistics will start from zero.{Colors.RESET}")
            
        except Exception as e:
            print(f"\n{Colors.ERROR}Failed to analyze model: {e}{Colors.RESET}")
            print(f"{Colors.WARNING}Proceeding with basic loading...{Colors.RESET}")
            starting_episode = 0
            has_optimizer = False
            has_metrics = False
        
        # Show training level options
        print(f"\n{Colors.INFO}Select training level to continue with:{Colors.RESET}")
        print(f"{Colors.SUCCESS}1{Colors.RESET} - Test Training (1,000 episodes)")
        print(f"{Colors.SUCCESS}2{Colors.RESET} - Small Training (10,000 episodes)")
        print(f"{Colors.SUCCESS}3{Colors.RESET} - Medium Training (100,000 episodes)")
        print(f"{Colors.SUCCESS}4{Colors.RESET} - Impossible Training (1,000,000 episodes)")
        
        level_choice = self.get_user_choice(
            f"\n{Colors.INFO}Select training level (1-4): {Colors.RESET}",
            ['1', '2', '3', '4']
        )
        
        level_configs = {
            '1': {'name': 'Test', 'episodes': 1000},
            '2': {'name': 'Small', 'episodes': 10000},
            '3': {'name': 'Medium', 'episodes': 100000},
            '4': {'name': 'Impossible', 'episodes': 1000000}
        }
        
        level_config = level_configs[level_choice]
        remaining_episodes = max(1000, level_config['episodes'] - starting_episode)
        
        print(f"\n{Colors.INFO}Training Configuration:{Colors.RESET}")
        print(f"Level: {Colors.SUCCESS}{level_config['name']}_FromModel{Colors.RESET}")
        print(f"Starting Episode: {Colors.WARNING}{starting_episode:,}{Colors.RESET}")
        print(f"Target Episodes: {Colors.WARNING}{level_config['episodes']:,}{Colors.RESET}")
        print(f"Additional Episodes: {Colors.WARNING}{remaining_episodes:,}{Colors.RESET}")
        
        confirm = self.get_user_choice(
            f"\n{Colors.WARNING}Start training from this model? (y/n): {Colors.RESET}",
            ['y', 'n', 'yes', 'no']
        ).lower()
        
        if confirm in ['n', 'no']:
            print(f"{Colors.INFO}Model loading cancelled.{Colors.RESET}")
            input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            return
        
        # Initialize trainer and load model
        try:
            print(f"\n{Colors.INFO}Initializing trainer with model...{Colors.RESET}")
            
            # Create trainer
            trainer = SingleEnvTrainer(self.config)
            
            # Load model
            print(f"{Colors.INFO}Loading model: {selected_model['name']}{Colors.RESET}")
            
            # Use CheckpointManager's load_checkpoint method for consistency
            model_dir = Path(selected_model['path']).parent
            model_checkpoint_manager = CheckpointManager(checkpoint_dir=model_dir)
            
            model_data = model_checkpoint_manager.load_checkpoint(
                checkpoint_path=selected_model['path'],
                agent=trainer.agent,
                optimizer=getattr(trainer.agent, 'optimizer', None) if has_optimizer else None,
                strict_loading=False,
                load_optimizer=has_optimizer
            )
            
            print(f"{Colors.SUCCESS}Model loaded successfully!{Colors.RESET}")
            print(f"{Colors.INFO}Loaded episode: {model_data.get('episode', 0):,}{Colors.RESET}")
            
            # Update trainer state if metrics are available
            if has_metrics:
                trainer.stats.win_stats.update(model_data.get('training_stats', {}))
                trainer.stats.ppo_metrics.update(model_data.get('training_metrics', {}))
                print(f"{Colors.SUCCESS}Training metrics restored.{Colors.RESET}")
            else:
                print(f"{Colors.WARNING}Starting with fresh training metrics.{Colors.RESET}")
            
            # Start training
            print(f"\n{Colors.SUCCESS}Starting continued training...{Colors.RESET}")
            input(f"{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            
            # Run training
            results = trainer.train(
                level_name=f"{level_config['name']}_FromModel",
                total_episodes=max(level_config['episodes'], starting_episode + remaining_episodes),
                show_game_render=(level_choice == '1'),
                render_interval=max(1, remaining_episodes // 10) if level_choice == '1' else remaining_episodes + 1
            )
            
            print(f"\n{Colors.SUCCESS}Continued training completed successfully!{Colors.RESET}")
            
        except Exception as e:
            print(f"\n{Colors.ERROR}Failed to load model for training: {e}{Colors.RESET}")
        
        input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
    
    def _view_available_checkpoints(self) -> None:
        """View detailed information about available checkpoints."""
        print(f"\n{Colors.HEADER}{'=' * 60}{Colors.RESET}")
        print(f"{Colors.BOLD}{Colors.INFO}>>> AVAILABLE CHECKPOINTS <<<{Colors.RESET}")
        print(f"{Colors.HEADER}{'=' * 60}{Colors.RESET}")
        
        # Search all training level directories
        from pathlib import Path
        import os
        
        models_dir = Path("models")
        if not models_dir.exists():
            print(f"\n{Colors.ERROR}Models directory not found: {models_dir}{Colors.RESET}")
            input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            return
        
        all_checkpoints = []
        training_levels = ['test', 'small', 'medium', 'impossible']
        
        for level in training_levels:
            level_dir = models_dir / level
            if level_dir.exists():
                checkpoint_manager = CheckpointManager(checkpoint_dir=level_dir)
                checkpoints = checkpoint_manager.list_checkpoints()
                for cp in checkpoints:
                    cp['training_level'] = level.title()
                all_checkpoints.extend(checkpoints)
        
        # Also check main models directory
        main_checkpoint_manager = CheckpointManager(checkpoint_dir=models_dir)
        main_checkpoints = main_checkpoint_manager.list_checkpoints()
        for cp in main_checkpoints:
            cp['training_level'] = 'General'
        all_checkpoints.extend(main_checkpoints)
        
        if not all_checkpoints:
            print(f"\n{Colors.ERROR}No checkpoints found in any training directories.{Colors.RESET}")
            print(f"{Colors.INFO}Available directories checked:{Colors.RESET}")
            for level in training_levels:
                level_dir = models_dir / level
                status = "âœ“" if level_dir.exists() else "âœ—"
                print(f"  {status} models/{level}/")
            print(f"  âœ“ models/")
            input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            return
        
        # Sort by episode (newest first)
        all_checkpoints.sort(key=lambda x: x.get('episode', 0), reverse=True)
        
        print(f"\n{Colors.INFO}Found {len(all_checkpoints)} checkpoint(s) across all training levels:{Colors.RESET}")
        print()
        
        # Display checkpoints with detailed info
        for i, checkpoint in enumerate(all_checkpoints, 1):
            episode = checkpoint.get('episode', 0)
            file_size = checkpoint.get('file_size_mb', 0)
            timestamp = checkpoint.get('timestamp', 0)
            is_best = checkpoint.get('is_best', False)
            training_level = checkpoint.get('training_level', 'Unknown')
            
            # Format timestamp
            import time
            time_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(timestamp)) if timestamp else 'Unknown'
            
            # Status indicators
            best_indicator = f"{Colors.SUCCESS}â­{Colors.RESET}" if is_best else "  "
            
            print(f"{best_indicator} {Colors.SUCCESS}{i:2}{Colors.RESET} - {Colors.INFO}{checkpoint['name']}{Colors.RESET}")
            print(f"      Level: {Colors.WARNING}{training_level}{Colors.RESET} | "
                  f"Episode: {Colors.WARNING}{episode:,}{Colors.RESET} | "
                  f"Size: {Colors.INFO}{file_size:.1f}MB{Colors.RESET}")
            print(f"      Date: {Colors.INFO}{time_str}{Colors.RESET} | "
                  f"Path: {Colors.INFO}{os.path.dirname(checkpoint['path'])}{Colors.RESET}")
            
            # Show performance summary if available
            perf_summary = checkpoint.get('performance_summary', {})
            if perf_summary:
                win_rate = perf_summary.get('win_rate', 0)
                total_games = perf_summary.get('total_games', 0)
                avg_reward = perf_summary.get('avg_reward', 0)
                
                if total_games > 0:
                    print(f"      Performance: Win Rate {Colors.SUCCESS}{win_rate:.1f}%{Colors.RESET} | "
                          f"Games: {Colors.INFO}{total_games:,}{Colors.RESET} | "
                          f"Avg Reward: {Colors.INFO}{avg_reward:.3f}{Colors.RESET}")
            
            print()
        
        input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
    
    def _show_training_configuration(self) -> None:
        """Show and modify training configuration settings."""
        print(f"\n{Colors.INFO}Training Configuration - Coming Soon!{Colors.RESET}")
        print(f"{Colors.INFO}This feature will allow modifying training hyperparameters.{Colors.RESET}")
        input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
    
    def _clear_training_data(self) -> None:
        """Clear training data with confirmation."""
        print(f"\n{Colors.ERROR}{Colors.BOLD}âš ï¸  CLEAR TRAINING DATA âš ï¸{Colors.RESET}")
        print(f"{Colors.WARNING}This will permanently delete all training checkpoints and models.{Colors.RESET}")
        print(f"{Colors.ERROR}This action cannot be undone!{Colors.RESET}")
        
        confirm1 = self.get_user_choice(
            f"\n{Colors.ERROR}Are you sure you want to clear all training data? (yes/no): {Colors.RESET}",
            ['yes', 'no']
        ).lower()
        
        if confirm1 != 'yes':
            print(f"{Colors.INFO}Clear operation cancelled.{Colors.RESET}")
            input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            return
        
        confirm2 = self.get_user_choice(
            f"{Colors.ERROR}Final confirmation - Type 'DELETE ALL' to proceed: {Colors.RESET}",
            ['DELETE ALL', 'cancel']
        )
        
        if confirm2 != 'DELETE ALL':
            print(f"{Colors.INFO}Clear operation cancelled.{Colors.RESET}")
            input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")
            return
        
        # Clear training data
        try:
            from pathlib import Path
            import shutil
            
            models_dir = Path("models")
            logs_dir = Path("logs")
            
            deleted_items = []
            
            # Remove models directory
            if models_dir.exists():
                shutil.rmtree(models_dir)
                deleted_items.append("models/")
            
            # Remove logs directory
            if logs_dir.exists():
                shutil.rmtree(logs_dir)
                deleted_items.append("logs/")
            
            if deleted_items:
                print(f"\n{Colors.SUCCESS}Successfully deleted:{Colors.RESET}")
                for item in deleted_items:
                    print(f"  âœ“ {item}")
            else:
                print(f"\n{Colors.INFO}No training data found to delete.{Colors.RESET}")
                
        except Exception as e:
            print(f"\n{Colors.ERROR}Failed to clear training data: {e}{Colors.RESET}")
        
        input(f"\n{Colors.WARNING}Press Enter to continue...{Colors.RESET}")


def main():
    """Main entry point."""
    try:
        interface = TrainingInterface()
        interface.run()
    except KeyboardInterrupt:
        print(f"\n\n{Colors.WARNING}Program interrupted. Goodbye!{Colors.RESET}")
    except Exception as e:
        print(f"{Colors.ERROR}An error occurred: {e}{Colors.RESET}")
        print("Please check your installation and try again.")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())